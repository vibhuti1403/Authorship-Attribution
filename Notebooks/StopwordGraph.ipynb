{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIvO6tKfYoOm",
        "outputId": "6025f062-cc84-4e1c-f91f-f7f15fa94871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "from copy import deepcopy\n",
        "import sklearn\n",
        "import ast\n",
        "import gensim\n",
        "from tensorflow import keras\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from scipy import spatial\n",
        "import operator\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PvPUHGrKYoOq"
      },
      "outputs": [],
      "source": [
        "#Import Datasets\n",
        "train = pd.read_json('/content/drive/MyDrive/sml-authorship-attribution/Data/train.json')\n",
        "test = pd.read_json('/content/drive/MyDrive/sml-authorship-attribution/Data/test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CXVsC0JYoOq",
        "outputId": "9380ecee-485c-403e-9833-c98e8047dea0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Identify stopwords as words that occur more than 20000 (number arrived by experimentation) in the entire corpus\n",
        "abstract_words=Counter([word for lst in train['abstract'].values for word in lst]).most_common()\n",
        "abstract_stop_words=[i[0] for i in abstract_words if i[1]>20000]\n",
        "len(abstract_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0yxU9n1xYoOs"
      },
      "outputs": [],
      "source": [
        "train['id']=train.index\n",
        "train['coauthors']=train['authors']\n",
        "train=train.explode('authors')\n",
        "train['coauthors']=train.apply(lambda x: set(x.coauthors)-set([x.authors]),axis=1)\n",
        "train=train[train['authors']<100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BnHjMg7YoOs",
        "outputId": "9d0af1e2-1430-4fcf-9d6c-a6f5d3a0030c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8938, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X_2g3eQdYoOt"
      },
      "outputs": [],
      "source": [
        "#Function to generate stopword graphs given a set of abstracts\n",
        "def get_stopword_graph(texts):\n",
        "    graph={}\n",
        "    for text in texts:\n",
        "        for i in abstract_stop_words:\n",
        "            if i in text:\n",
        "                if i not in graph.keys():\n",
        "                    graph[i]={}\n",
        "                for ind in [k for k,val in enumerate(text) if val==i]:\n",
        "                    for j in set(abstract_stop_words)-set([i]):\n",
        "                        if (j in text):\n",
        "                            indices_j=[k for k,val in enumerate(text) if val==j]\n",
        "                            distances=[abs(l-ind) for l in indices_j]\n",
        "                            dist=np.exp(-abs(indices_j[distances.index(min(distances))]-ind))\n",
        "                            if j in graph[i].keys():\n",
        "                                graph[i][j]+=dist\n",
        "                            else:\n",
        "                                graph[i][j]=dist\n",
        "    for i in graph.keys():\n",
        "        for j in graph[i].keys():\n",
        "            if j in graph.keys():\n",
        "                graph[j][i]=graph[i][j]\n",
        "            else:\n",
        "                graph[j]={}\n",
        "                graph[j][i]=graph[i][j]\n",
        "    for i in abstract_stop_words:\n",
        "        for j in abstract_stop_words:\n",
        "            if i not in graph.keys():\n",
        "                graph[i]={}\n",
        "                graph[i][j]=0\n",
        "                graph[j][i]=0\n",
        "            if j not in graph.keys():\n",
        "                graph[j]={}\n",
        "                graph[j][i]=0\n",
        "                graph[i][j]=0\n",
        "            if j not in graph[i].keys():\n",
        "                graph[i][j]=0\n",
        "                graph[j][i]=0\n",
        "    return graph\n",
        "#Normalise the weights of the graph\n",
        "def normalise_graph(graph):\n",
        "    for i in graph.keys():\n",
        "        sum=0\n",
        "        for j in graph[i].keys():\n",
        "            sum+=graph[i][j]\n",
        "        if sum!=0:\n",
        "            for j in graph[i].keys():\n",
        "                graph[i][j]=graph[i][j]/sum\n",
        "    return graph\n",
        "#Calculate the kulback leiber divergence of two vectors\n",
        "def kl_div(p,q):\n",
        "    kl1=0\n",
        "    kl2=0\n",
        "    for i in p.keys():\n",
        "        if p[i]!=0:\n",
        "            if q[i]!=0:\n",
        "                kl1+=p[i]*math.log(p[i]/q[i])\n",
        "                kl2+=q[i]*math.log(q[i]/p[i])\n",
        "    return (kl1+kl2)/2           \n",
        "#Calculate the kulback leiber divergence of two graphs\n",
        "def get_KL_divergence(G1,G2):\n",
        "    G1_tmp=deepcopy(G1)\n",
        "    for i in G2.keys():\n",
        "        for j in G2.keys():\n",
        "            if G2[i][j]==0:\n",
        "                G1_tmp[i][j]=0\n",
        "                G1_tmp[j][i]=0\n",
        "    G1_tmp=normalise_graph(G1_tmp)\n",
        "    G2=normalise_graph(G2)\n",
        "    kl_divergence=0\n",
        "    for i in G2.keys():\n",
        "        kl_divergence+=kl_div(G1_tmp[i],G2[i])\n",
        "    return kl_divergence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K_wDeMEgYoOv"
      },
      "outputs": [],
      "source": [
        "is_train=False\n",
        "if is_train:\n",
        "    author_df=train.groupby(['authors','year']).sample(frac=0.8, random_state=1).groupby('authors').agg(stopword_graph=('abstract',get_stopword_graph)).reset_index()\n",
        "else:\n",
        "    author_df=train.groupby('authors').agg(stopword_graph=('abstract',get_stopword_graph)).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ESRpBb7fYoOv"
      },
      "outputs": [],
      "source": [
        "#Import Search space generated by the coauthor graph\n",
        "test_auth=pd.read_csv('/content/drive/MyDrive/sml-authorship-attribution/test_probable_authors_df_2_test.csv')\n",
        "tmp_test=test.merge(test_auth[['identifier','probable_auth']],on=['identifier'],how='left')\n",
        "tmp_test.probable_auth=tmp_test.probable_auth.fillna('[]')\n",
        "tmp_test.probable_auth=tmp_test.probable_auth.apply(lambda s: list(ast.literal_eval(s)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OpoJxcd0YoOw"
      },
      "outputs": [],
      "source": [
        "#For each paper in the test set calculate the top 'n' most similar authors in the search space\n",
        "res=[]\n",
        "for i in range(0,tmp_test.shape[0]):\n",
        "    lst={}\n",
        "    potential_authors=[i for i in tmp_test.loc[i].probable_auth if i!=100]\n",
        "    if len(potential_authors)==0:\n",
        "        potential_authors=range(0,100)\n",
        "    tmp=get_stopword_graph([tmp_test.loc[i].abstract])\n",
        "    for author in potential_authors:\n",
        "        lst[author]=get_KL_divergence(author_df[author_df.authors==author].stopword_graph.values[0],tmp)\n",
        "    lst=lst=dict(sorted(lst.items(),key=operator.itemgetter(1)))\n",
        "    # print({'identifier':i,'authors':lst})\n",
        "    res.append({'identifier':i,'authors':list(lst.keys())[0:3]})\n",
        "res_df=pd.DataFrame(data=res,columns=['identifier','authors'])\n",
        "res_df['Predict'] = [' '.join(map(str, l)) for l in res_df['authors']]\n",
        "res_df['ID']=res_df['identifier']\n",
        "res_df[['ID','Predict']].to_csv('submission_classifier_1_hop.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSvGuStoaywe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "b2c1ab0a207ce9574c3b3a9adf0f8bbfcb7bceef9141640414cce65460e53b6a"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}